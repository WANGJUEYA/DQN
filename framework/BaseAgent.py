from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional

import torch

from framework.convergence_analysis import ConvergenceAnalyzer


class BaseAgent(ABC):
    """åŸºç¡€æ™ºèƒ½ä½“ç±»ï¼Œå°è£…è®­ç»ƒã€æ¨ç†ã€æ¨¡å‹ç®¡ç†ã€åˆ†æç­‰é€šç”¨é€»è¾‘"""

    def __init__(self, output_dir="outputs", model_dir="models", training_mode=False):
        self.output_dir = Path(output_dir)
        self.model_dir = Path(model_dir)
        self.convergence_analyzer = ConvergenceAnalyzer()
        self._init_env_and_agent()
        self.training_number = None
        if training_mode:
            self.training_number = self._get_training_number_and_create_output_dir()
        else:
            # éè®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ‰¾åˆ°æœ€æ–°çš„è®­ç»ƒç¼–å·å’Œè¾“å‡ºç›®å½•
            game_output_root = self.output_dir / self.game_name
            if game_output_root.exists():
                all_dirs = [d for d in game_output_root.iterdir() if d.is_dir() and d.name.isdigit()]
                if all_dirs:
                    latest = max(all_dirs, key=lambda d: int(d.name))
                    self.game_output_dir = latest
                else:
                    self.game_output_dir = game_output_root / "1"
            else:
                self.game_output_dir = game_output_root / "1"

    @abstractmethod
    def _init_env_and_agent(self):
        """åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“è‡ªèº«ï¼Œéœ€è®¾ç½®self.env, self.agent, self._max_steps, self._game_name"""
        pass

    @abstractmethod
    def _is_success(self, episode: int, steps: int, reward: float) -> bool:
        pass

    @abstractmethod
    def _get_max_steps(self) -> int:
        pass

    @property
    @abstractmethod
    def game_name(self) -> str:
        pass

    def _get_training_number_and_create_output_dir(self) -> int:
        game_output_root = self.output_dir / self.game_name
        game_output_root.mkdir(parents=True, exist_ok=True)
        max_count = 0
        for d in game_output_root.iterdir():
            if d.is_dir() and d.name.isdigit():
                max_count = max(max_count, int(d.name))
        count = max_count + 1
        self.game_output_dir = game_output_root / str(count)
        self.game_output_dir.mkdir(parents=True, exist_ok=True)
        (self.game_output_dir / "reports").mkdir(exist_ok=True)
        return count

    def _get_training_number(self) -> int:
        max_training_number = 0
        if self.model_dir.exists():
            pattern = f"{self.game_name}_dqn_training_*_*.pt"
            training_models = list(self.model_dir.glob(pattern))
            for model_file in training_models:
                parts = model_file.stem.split('_')
                if len(parts) >= 4 and parts[0] == self.game_name and parts[1] == "dqn" and parts[2] == "training":
                    try:
                        training_number = int(parts[3])
                        max_training_number = max(max_training_number, training_number)
                    except ValueError:
                        continue
        return max_training_number + 1

    def train(self, episodes: int, save_interval: int = 50, model_name: Optional[str] = None, render: bool = False):
        import time
        print(f"å¼€å§‹è®­ç»ƒ {self.game_name} æ¨¡å‹...")
        print(f"è®­ç»ƒå‚æ•°: episodes={episodes}, save_interval={save_interval}")
        if self.training_number is None:
            self.training_number = self._get_training_number_and_create_output_dir()
        training_number = self.training_number
        print(f"è¿™æ˜¯ç¬¬ {training_number} æ¬¡è®­ç»ƒ")
        best_reward = float('-inf')
        best_episode = 0
        best_model_path = None  # åˆå§‹åŒ–best_model_path
        max_steps = self._get_max_steps()

        train_start_time = time.time()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        try:
            for episode in range(episodes):
                reset_result = self.env.reset()
                if isinstance(reset_result, tuple):
                    state = reset_result[0]
                else:
                    state = reset_result
                total_reward = 0
                total_loss = 0
                steps = 0
                success = False

                while True:
                    # ä¼˜åŒ–æ¸²æŸ“é€»è¾‘ï¼Œé¿å…é˜»å¡
                    if render:
                        try:
                            self.env.render()
                        except Exception as render_error:
                            print(f"  âš ï¸ æ¸²æŸ“é”™è¯¯: {render_error}")
                            render = False  # ç¦ç”¨æ¸²æŸ“ä»¥é¿å…åç»­é”™è¯¯

                    action = self.agent.choose_action(state)
                    step_result = self.env.step(action)
                    if len(step_result) == 5:
                        next_state, reward, terminated, truncated, info = step_result
                        done = terminated or truncated
                    else:
                        next_state, reward, done, info = step_result

                    self.agent.store_transition(state, action, reward, next_state)
                    if self.agent.point > 32:
                        loss = self.agent.learn()
                        total_loss += loss

                    state = next_state
                    total_reward += reward
                    steps += 1

                    # ä½¿ç”¨åŸæ¥çš„æœ€å¤§æ­¥æ•°é™åˆ¶
                    if steps >= max_steps:
                        done = True
                        print(f"  Episode {episode + 1} è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ ({max_steps})ï¼Œå¼ºåˆ¶ç»“æŸ")

                    # å‡å°‘æ‰“å°é¢‘ç‡ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                    if steps % 200 == 0:
                        current_loss = total_loss / max(steps, 1)
                        print(
                            f"  Episode {episode + 1}, Step {steps}: Reward={total_reward:.2f}, Loss={current_loss:.4f}")

                    if done:
                        success = self._is_success(episode, steps, total_reward)
                        break

                avg_loss = total_loss / max(steps, 1)

                self.convergence_analyzer.add_episode_data(
                    episode=episode,
                    reward=total_reward,
                    loss=avg_loss,
                    steps=steps,
                    success=success,
                    epsilon=0.9
                )

                print(f"Episode {episode + 1}/{episodes}: "
                      f"Reward={total_reward:.2f}, "
                      f"Steps={steps}, "
                      f"AvgLoss={avg_loss:.4f}, "
                      f"Success={success}")

                if total_reward > best_reward:
                    best_reward = total_reward
                    best_episode = episode + 1
                    # ä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°modelsç›®å½•
                    best_model_name = f"{self.game_name}_dqn_training_{training_number}_best.pt"
                    best_model_path = self.model_dir / best_model_name
                    self.agent.save_model(str(best_model_path))
                    checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
                    checkpoint['best_reward'] = best_reward
                    checkpoint['best_episode'] = best_episode
                    checkpoint['training_number'] = training_number
                    torch.save(checkpoint, best_model_path)
                    print(f"  ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Episode {episode + 1}, Reward: {total_reward:.2f}")

                if (episode + 1) % save_interval == 0:
                    self._save_checkpoint(episode, training_number)
                    self._generate_reports(episode, train_start_time)

            final_model_name = f"{self.game_name}_dqn_training_{training_number}_final.pt"
            final_model_path = self.model_dir / final_model_name
            self.agent.save_model(str(final_model_path))
            # æ›´æ–°å…¨å±€æœ€ä¼˜æ¨¡å‹
            if best_model_path is not None:  # ç¡®ä¿æœ‰æœ€ä¼˜æ¨¡å‹å­˜åœ¨
                global_best_model_path = self.model_dir / f"{self.game_name}_dqn_best.pt"
                current_global_best_reward = self._get_global_best_reward()
                if best_reward > current_global_best_reward:
                    import shutil
                    shutil.copy2(str(best_model_path), str(global_best_model_path))
                    print(f"  ğŸ† æ›´æ–°å…¨å±€æœ€ä¼˜æ¨¡å‹ï¼Reward: {best_reward:.2f}")
            self._save_checkpoint(episodes - 1, training_number)
            self._generate_reports(episodes - 1, train_start_time)
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜å½“å‰è¿›åº¦
            if best_model_path is not None:
                print(f"å·²ä¿å­˜æœ€ä¼˜æ¨¡å‹: {best_model_path}")
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        finally:
            print(f"âœ… è®­ç»ƒå®Œæˆï¼")
            print(f"æœ€ä¼˜å¥–åŠ±: {best_reward:.2f} (Episode {best_episode})")
            print(f"è¾“å‡ºç›®å½•: {self.game_output_dir}")

    def _get_global_best_reward(self) -> float:
        global_best_model_path = self.model_dir / f"{self.game_name}_dqn_best.pt"
        if global_best_model_path.exists():
            try:
                checkpoint = torch.load(global_best_model_path, map_location='cpu', weights_only=False)
                return checkpoint.get('best_reward', float('-inf'))
            except:
                return float('-inf')
        return float('-inf')

    def _save_checkpoint(self, episode: int, training_number: int):
        convergence_dir = self.game_output_dir / "convergence_analysis"
        convergence_dir.mkdir(exist_ok=True)

        # ä¿å­˜æ”¶æ•›åˆ†ææ•°æ®
        convergence_file = convergence_dir / f"convergence_data_{episode}.json"
        self.convergence_analyzer.save_analysis_data(str(convergence_file))

        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹åˆ°outputsç›®å½•ï¼ˆä¸åœ¨modelsç›®å½•ï¼‰
        checkpoint_model_name = f"{self.game_name}_dqn_training_{training_number}_checkpoint_{episode}.pt"
        checkpoint_model_path = self.game_output_dir / "process_models" / checkpoint_model_name
        checkpoint_model_path.parent.mkdir(exist_ok=True)
        self.agent.save_model(str(checkpoint_model_path))

        # ä¿å­˜æ£€æŸ¥ç‚¹å…ƒæ•°æ®
        checkpoint_metadata = {
            'episode': episode,
            'training_number': training_number,
            'game_name': self.game_name,
            'convergence_file': str(convergence_file.relative_to(self.game_output_dir))
        }
        checkpoint_metadata_file = convergence_dir / f"checkpoint_metadata_{episode}.json"
        import json
        with open(checkpoint_metadata_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_metadata, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: Episode {episode + 1}")

    def _generate_reports(self, episode: int, train_start_time=None):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        import time
        reports_dir = self.game_output_dir / "reports"
        reports_dir.mkdir(exist_ok=True)

        # è®¡ç®—è®­ç»ƒæ—¶é—´
        total_seconds = None
        if train_start_time is not None:
            total_seconds = time.time() - train_start_time

        # ç”Ÿæˆæ”¶æ•›åˆ†ææŠ¥å‘Š
        report_path = reports_dir / f"convergence_report_{episode}.txt"
        self.convergence_analyzer.generate_convergence_report(str(report_path), total_seconds=total_seconds)

        # ç”Ÿæˆæ”¶æ•›åˆ†æå›¾è¡¨
        plot_path = reports_dir / f"convergence_plot_{episode}.png"
        print(f"  ğŸ“Š æ­£åœ¨ç”Ÿæˆå›¾è¡¨: {plot_path}")
        try:
            self.convergence_analyzer.plot_convergence_analysis(str(plot_path), show_plot=False,
                                                                total_seconds=total_seconds)
            if plot_path.exists():
                print(f"  âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: {plot_path}")
            else:
                print(f"  âŒ å›¾è¡¨æ–‡ä»¶æœªç”Ÿæˆ: {plot_path}")
        except Exception as e:
            print(f"  âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

        print(f"  ğŸ“Š ç”ŸæˆæŠ¥å‘Š: Episode {episode + 1}")

    def inference(self, model_name: Optional[str] = None, episodes: int = 5):
        """æ¨¡å‹æ¨ç†"""
        print(f"å¼€å§‹æ¨ç† {self.game_name} æ¨¡å‹...")

        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        if model_name is None:
            best_model = self._get_best_model()
            if best_model is None:
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
                return
            model_path = self.model_dir / best_model
        else:
            model_path = self.model_dir / model_name
            if not model_path.exists():
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return

        print(f"ä½¿ç”¨æ¨¡å‹: {model_path.name}")

        # åŠ è½½æ¨¡å‹
        try:
            self.agent.load_model(str(model_path))
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return

        # æ‰§è¡Œæ¨ç†
        total_rewards = []
        total_steps = []
        success_count = 0

        for episode in range(episodes):
            reset_result = self.env.reset()
            if isinstance(reset_result, tuple):
                state = reset_result[0]
            else:
                state = reset_result

            total_reward = 0
            steps = 0
            max_steps = self._get_max_steps()

            while True:
                self.env.render()
                action = self.agent.predict_action(state, epsilon=0.0)  # æ¨ç†æ—¶ä¸ä½¿ç”¨éšæœºæ¢ç´¢
                step_result = self.env.step(action)

                if len(step_result) == 5:
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    next_state, reward, done, info = step_result

                state = next_state
                total_reward += reward
                steps += 1

                if steps >= max_steps:
                    done = True
                    print(f"  Episode {episode + 1} è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ ({max_steps})ï¼Œå¼ºåˆ¶ç»“æŸ")

                if done:
                    success = self._is_success(episode, steps, total_reward)
                    break

            total_rewards.append(total_reward)
            total_steps.append(steps)
            if success:
                success_count += 1

            print(f"Episode {episode + 1}/{episodes}: "
                  f"Reward={total_reward:.2f}, "
                  f"Steps={steps}, "
                  f"Success={success}")

        # è¾“å‡ºæ¨ç†ç»Ÿè®¡
        avg_reward = sum(total_rewards) / len(total_rewards)
        avg_steps = sum(total_steps) / len(total_steps)
        success_rate = success_count / episodes

        print(f"\nğŸ“Š æ¨ç†ç»Ÿè®¡:")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        print(f"æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{episodes})")
        print(f"âœ… æ¨ç†å®Œæˆï¼")

    def list_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
        print(f"ğŸ“ {self.game_name} æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:")
        print(f"æ¨¡å‹ç›®å½•: {self.model_dir}")

        if not self.model_dir.exists():
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return

        models = list(self.model_dir.glob("*.pt"))
        if not models:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            return

        # åˆ†ç±»æ˜¾ç¤ºæ¨¡å‹
        global_models = [m for m in models if not m.name.startswith(f"{self.game_name}_dqn_training_")]
        training_models = [m for m in models if m.name.startswith(f"{self.game_name}_dqn_training_")]

        if global_models:
            print("\nğŸŒ å…¨å±€æ¨¡å‹:")
            for model in sorted(global_models):
                size_mb = model.stat().st_size / (1024 * 1024)
                print(f"  ğŸ“„ {model.name} ({size_mb:.1f} MB)")

        if training_models:
            print("\nğŸ¯ è®­ç»ƒæ¨¡å‹:")
            # æŒ‰è®­ç»ƒç¼–å·åˆ†ç»„
            training_groups = {}
            for model in training_models:
                parts = model.stem.split('_')
                if len(parts) >= 4:
                    training_num = parts[3]
                    if training_num not in training_groups:
                        training_groups[training_num] = []
                    training_groups[training_num].append(model)

            for training_num in sorted(training_groups.keys(), key=int):
                print(f"  è®­ç»ƒ #{training_num}:")
                for model in sorted(training_groups[training_num]):
                    size_mb = model.stat().st_size / (1024 * 1024)
                    print(f"    ğŸ“„ {model.name} ({size_mb:.1f} MB)")

    def list_outputs(self):
        """åˆ—å‡ºè®­ç»ƒè¾“å‡ºæ–‡ä»¶"""
        print(f"ğŸ“ {self.game_name} è¾“å‡ºæ–‡ä»¶åˆ—è¡¨:")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

        game_output_root = self.output_dir / self.game_name
        if not game_output_root.exists():
            print("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
            return

        training_dirs = [d for d in game_output_root.iterdir() if d.is_dir() and d.name.isdigit()]
        if not training_dirs:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒè¾“å‡º")
            return

        print(f"\nğŸ¯ è®­ç»ƒè¾“å‡º (å…± {len(training_dirs)} æ¬¡):")
        for training_dir in sorted(training_dirs, key=lambda x: int(x.name)):
            print(f"\n  è®­ç»ƒ #{training_dir.name}:")
            print(f"    è·¯å¾„: {training_dir}")

            # æ£€æŸ¥å­ç›®å½•
            subdirs = ['reports', 'convergence_analysis', 'process_models']
            for subdir in subdirs:
                subdir_path = training_dir / subdir
                if subdir_path.exists():
                    files = list(subdir_path.iterdir())
                    if files:
                        print(f"    ğŸ“ {subdir}: {len(files)} ä¸ªæ–‡ä»¶")
                        for file in sorted(files)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                            print(f"      ğŸ“„ {file.name}")
                        if len(files) > 3:
                            print(f"      ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")

    def _get_best_model(self) -> Optional[str]:
        """è·å–æœ€ä¼˜æ¨¡å‹æ–‡ä»¶å"""
        # é¦–å…ˆå°è¯•å…¨å±€æœ€ä¼˜æ¨¡å‹
        best_model_path = self.model_dir / f"{self.game_name}_dqn_best.pt"
        if best_model_path.exists():
            return best_model_path.name

        # ç„¶åå°è¯•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä¼˜æ¨¡å‹
        pattern = f"{self.game_name}_dqn_training_*_best.pt"
        best_models = list(self.model_dir.glob(pattern))
        if best_models:
            # æŒ‰è®­ç»ƒç¼–å·æ’åºï¼Œå–æœ€æ–°çš„
            best_models.sort(key=lambda x: int(x.stem.split('_')[3]))
            return best_models[-1].name

        # æœ€åå°è¯•æœ€ç»ˆæ¨¡å‹
        final_model_path = self.model_dir / f"{self.game_name}_dqn_final.pt"
        if final_model_path.exists():
            return final_model_path.name

        return None

    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        model_dir = Path(path).parent
        model_dir.mkdir(parents=True, exist_ok=True)

        torch.save({
            'evaluate_net_state_dict': self.evaluate_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'game_name': self.game_name,
        }, path)

    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location='cpu')
        self.evaluate_net.load_state_dict(checkpoint['evaluate_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
