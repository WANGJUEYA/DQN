#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒç›‘æ§æ¨¡å—
ç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„èµ„æºä½¿ç”¨ã€æ€§èƒ½å’Œç¨³å®šæ€§
"""

import time
import psutil
import threading
import warnings
from typing import Optional, Callable
from dataclasses import dataclass
from pathlib import Path

@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡æ•°æ®ç±»"""
    episode: int
    reward: float
    loss: float
    steps: int
    episode_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    gpu_usage_percent: Optional[float] = None

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    
    def __init__(self, output_dir: str = "outputs", log_interval: int = 10):
        self.output_dir = Path(output_dir)
        self.log_interval = log_interval
        self.metrics_history = []
        self.start_time = None
        self.monitoring = False
        self.monitor_thread = None
        
        # åˆ›å»ºç›‘æ§è¾“å‡ºç›®å½•
        self.monitor_dir = self.output_dir / "training_monitor"
        self.monitor_dir.mkdir(parents=True, exist_ok=True)
        
        # æ€§èƒ½è­¦å‘Šé˜ˆå€¼
        self.memory_warning_threshold = 80.0  # å†…å­˜ä½¿ç”¨ç‡è­¦å‘Šé˜ˆå€¼(%)
        self.cpu_warning_threshold = 90.0     # CPUä½¿ç”¨ç‡è­¦å‘Šé˜ˆå€¼(%)
        self.episode_timeout = 300            # Episodeè¶…æ—¶æ—¶é—´(ç§’)
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_resources, daemon=True)
        self.monitor_thread.start()
        print("ğŸ” è®­ç»ƒç›‘æ§å·²å¯åŠ¨")
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        print("ğŸ” è®­ç»ƒç›‘æ§å·²åœæ­¢")
        
    def _monitor_resources(self):
        """èµ„æºç›‘æ§çº¿ç¨‹"""
        while self.monitoring:
            try:
                # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
                memory_percent = psutil.virtual_memory().percent
                cpu_percent = psutil.cpu_percent(interval=1)
                
                # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
                gpu_percent = None
                try:
                    import torch
                    if torch.cuda.is_available():
                        gpu_percent = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
                except:
                    pass
                
                # æ£€æŸ¥èµ„æºä½¿ç”¨æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                if memory_percent > self.memory_warning_threshold:
                    print(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
                    
                if cpu_percent > self.cpu_warning_threshold:
                    print(f"âš ï¸ CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent:.1f}%")
                    
                time.sleep(self.log_interval)
                
            except Exception as e:
                print(f"âš ï¸ èµ„æºç›‘æ§é”™è¯¯: {e}")
                time.sleep(self.log_interval)
                
    def add_episode_metrics(self, metrics: TrainingMetrics):
        """æ·»åŠ episodeæŒ‡æ ‡"""
        self.metrics_history.append(metrics)
        
        # æ£€æŸ¥episodeæ—¶é—´æ˜¯å¦è¶…æ—¶
        if metrics.episode_time > self.episode_timeout:
            print(f"âš ï¸ Episode {metrics.episode} è€—æ—¶è¿‡é•¿: {metrics.episode_time:.1f}s")
            
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        if metrics.memory_usage_mb > 1000:  # è¶…è¿‡1GB
            print(f"âš ï¸ Episode {metrics.episode} å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics.memory_usage_mb:.1f}MB")
            
    def get_current_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        return psutil.Process().memory_info().rss / 1024 / 1024
        
    def get_current_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡(%)"""
        return psutil.cpu_percent()
        
    def get_current_gpu_usage(self) -> Optional[float]:
        """è·å–å½“å‰GPUä½¿ç”¨ç‡(%)"""
        try:
            import torch
            if torch.cuda.is_available():
                return torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
        except:
            pass
        return None
        
    def generate_monitoring_report(self, filename: str = "training_monitor_report.txt"):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        if not self.metrics_history:
            print("âš ï¸ æ²¡æœ‰ç›‘æ§æ•°æ®å¯ç”ŸæˆæŠ¥å‘Š")
            return
            
        report_path = self.monitor_dir / filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("è®­ç»ƒç›‘æ§æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            total_episodes = len(self.metrics_history)
            total_time = time.time() - self.start_time if self.start_time else 0
            
            f.write(f"æ€»è®­ç»ƒepisodes: {total_episodes}\n")
            f.write(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’\n")
            f.write(f"å¹³å‡episodeæ—¶é—´: {total_time/total_episodes:.1f}ç§’\n\n")
            
            # èµ„æºä½¿ç”¨ç»Ÿè®¡
            memory_usage = [m.memory_usage_mb for m in self.metrics_history]
            cpu_usage = [m.cpu_usage_percent for m in self.metrics_history]
            
            f.write("èµ„æºä½¿ç”¨ç»Ÿè®¡:\n")
            f.write(f"  å†…å­˜ä½¿ç”¨ - å¹³å‡: {sum(memory_usage)/len(memory_usage):.1f}MB, "
                   f"æœ€å¤§: {max(memory_usage):.1f}MB\n")
            f.write(f"  CPUä½¿ç”¨ - å¹³å‡: {sum(cpu_usage)/len(cpu_usage):.1f}%, "
                   f"æœ€å¤§: {max(cpu_usage):.1f}%\n\n")
            
            # æ€§èƒ½è­¦å‘Šç»Ÿè®¡
            timeout_episodes = [m for m in self.metrics_history if m.episode_time > self.episode_timeout]
            high_memory_episodes = [m for m in self.metrics_history if m.memory_usage_mb > 1000]
            
            f.write("æ€§èƒ½è­¦å‘Šç»Ÿè®¡:\n")
            f.write(f"  è¶…æ—¶episodes: {len(timeout_episodes)}\n")
            f.write(f"  é«˜å†…å­˜ä½¿ç”¨episodes: {len(high_memory_episodes)}\n\n")
            
            # è¯¦ç»†episodeæ•°æ®
            f.write("è¯¦ç»†episodeæ•°æ®:\n")
            f.write("Episode | å¥–åŠ± | æŸå¤± | æ­¥æ•° | æ—¶é—´(s) | å†…å­˜(MB) | CPU(%)\n")
            f.write("-" * 70 + "\n")
            
            for metrics in self.metrics_history[-20:]:  # åªæ˜¾ç¤ºæœ€å20ä¸ªepisodes
                f.write(f"{metrics.episode:7d} | {metrics.reward:6.1f} | {metrics.loss:6.4f} | "
                       f"{metrics.steps:4d} | {metrics.episode_time:7.1f} | "
                       f"{metrics.memory_usage_mb:8.1f} | {metrics.cpu_usage_percent:5.1f}\n")
        
        print(f"ğŸ“Š ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()
        
        # æ¸…ç†è¿‡æœŸçš„ç›‘æ§æ•°æ®
        if len(self.metrics_history) > 1000:  # ä¿ç•™æœ€è¿‘1000ä¸ªepisodesçš„æ•°æ®
            self.metrics_history = self.metrics_history[-1000:]

def create_training_monitor(output_dir: str = "outputs") -> TrainingMonitor:
    """åˆ›å»ºè®­ç»ƒç›‘æ§å™¨å®ä¾‹"""
    return TrainingMonitor(output_dir)

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_global_monitor: Optional[TrainingMonitor] = None

def get_global_monitor() -> TrainingMonitor:
    """è·å–å…¨å±€ç›‘æ§å™¨å®ä¾‹"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = create_training_monitor()
    return _global_monitor

def start_global_monitoring():
    """å¯åŠ¨å…¨å±€ç›‘æ§"""
    monitor = get_global_monitor()
    monitor.start_monitoring()

def stop_global_monitoring():
    """åœæ­¢å…¨å±€ç›‘æ§"""
    global _global_monitor
    if _global_monitor:
        _global_monitor.stop_monitoring()
        _global_monitor.generate_monitoring_report()
        _global_monitor.cleanup()
        _global_monitor = None 