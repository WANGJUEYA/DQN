#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DQNé¡¹ç›®ä¸»ç¨‹åº
æ”¯æŒå¤šç§æ¸¸æˆç¯å¢ƒçš„è®­ç»ƒå’Œæ¨ç†
"""

import argparse
import os
import sys
import json
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from games.Maze.MazeAgent import MazeAgent
from games.CartPole.CartPole import DQN
from framework.convergence_analysis import ConvergenceAnalyzer
from framework.plot_convergence import plot_convergence_data


def start_training(game_type, episodes=1000, save_interval=50, output_dir="outputs", model_dir="models", render=False):
    """
    å¯åŠ¨æ¨¡å‹è®­ç»ƒ
    
    Args:
        game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
        episodes (int): è®­ç»ƒepisodeæ•°é‡
        save_interval (int): ä¿å­˜é—´éš”
        output_dir (str): è¾“å‡ºç›®å½•
        model_dir (str): æ¨¡å‹ç›®å½•
        render (bool): æ˜¯å¦åœ¨è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£
    """
    print(f"ğŸš€ å¯åŠ¨ {game_type} æ¨¡å‹è®­ç»ƒ...")
    project = DQNProject(game_type, output_dir, model_dir)
    project.train(episodes, save_interval, render=render)
    print(f"âœ… {game_type} æ¨¡å‹è®­ç»ƒå®Œæˆï¼")


def start_inference(game_type, model_name=None, episodes=5, output_dir="outputs", model_dir="models"):
    """
    å¯åŠ¨æ¨¡å‹æ¨ç†
    
    Args:
        game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
        model_name (str): æ¨¡å‹æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä¼˜æ¨¡å‹
        episodes (int): æ¨ç†episodeæ•°é‡
        output_dir (str): è¾“å‡ºç›®å½•
        model_dir (str): æ¨¡å‹ç›®å½•
    """
    print(f"ğŸ” å¯åŠ¨ {game_type} æ¨¡å‹æ¨ç†...")
    project = DQNProject(game_type, output_dir, model_dir)
    project.inference(model_name, episodes)
    print(f"âœ… {game_type} æ¨¡å‹æ¨ç†å®Œæˆï¼")


class DQNProject:
    """DQNé¡¹ç›®ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, game_type, output_dir="outputs", model_dir="models"):
        """
        åˆå§‹åŒ–é¡¹ç›®æ§åˆ¶å™¨
        
        Args:
            game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
            output_dir (str): è¾“å‡ºç›®å½•
            model_dir (str): æ¨¡å‹ç›®å½•
        """
        self.game_type = game_type.lower()
        self.output_dir = Path(output_dir)
        self.model_dir = Path(model_dir)
        
        # è·å–è®­ç»ƒç¼–å·ï¼ˆcountï¼‰ï¼Œå¹¶åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.training_number = self._get_training_number_and_create_output_dir()
        
        # åˆå§‹åŒ–æ¸¸æˆç‰¹å®šçš„ç»„ä»¶
        self._init_game_components()
        
        # åˆå§‹åŒ–æ”¶æ•›åˆ†æå™¨
        self.convergence_analyzer = ConvergenceAnalyzer()
        
    def _get_training_number_and_create_output_dir(self):
        """è·å–å½“å‰è®­ç»ƒç¼–å·ï¼Œå¹¶åˆ›å»ºoutputs/{gameName}/{count}ç›®å½•"""
        game_output_root = self.output_dir / self.game_type
        game_output_root.mkdir(parents=True, exist_ok=True)
        # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—å­ç›®å½•
        max_count = 0
        for d in game_output_root.iterdir():
            if d.is_dir() and d.name.isdigit():
                max_count = max(max_count, int(d.name))
        count = max_count + 1
        # åˆ›å»ºæœ¬æ¬¡è¾“å‡ºç›®å½•
        self.game_output_dir = game_output_root / str(count)
        self.game_output_dir.mkdir(parents=True, exist_ok=True)
        # åªåˆ›å»ºreportsç›®å½•ï¼Œå…¶ä»–ç›®å½•åœ¨éœ€è¦æ—¶åˆ›å»º
        (self.game_output_dir / "reports").mkdir(exist_ok=True)
        return count
        
    def _init_game_components(self):
        """åˆå§‹åŒ–æ¸¸æˆç‰¹å®šçš„ç»„ä»¶"""
        if self.game_type == "maze":
            from games.Maze.MazeEnv import DEFAULT_MAZE, MazeEnv
            self.env = MazeEnv(DEFAULT_MAZE)
            self.agent = MazeAgent()
            self.game_name = "Maze"
        elif self.game_type == "cartpole":
            import gymnasium as gym
            # CartPoleç¯å¢ƒéœ€è¦æŒ‡å®šæ¸²æŸ“æ¨¡å¼
            self.env = gym.make('CartPole-v1', render_mode='human').unwrapped
            self.agent = DQN()
            self.game_name = "CartPole"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¸¸æˆç±»å‹: {self.game_type}")
            
    def _get_training_number(self):
        """è·å–å½“å‰è®­ç»ƒæ¬¡æ•°å¹¶é€’å¢"""
        # ä»modelsç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶åä¸­è·å–å½“å‰æ¸¸æˆçš„æœ€å¤§è®­ç»ƒæ¬¡æ•°
        max_training_number = 0
        
        if self.model_dir.exists():
            # æŸ¥æ‰¾æ‰€æœ‰å½“å‰æ¸¸æˆçš„è®­ç»ƒæ¨¡å‹æ–‡ä»¶
            pattern = f"{self.game_type}_dqn_training_*_*.pth"
            training_models = list(self.model_dir.glob(pattern))
            
            for model_file in training_models:
                # ä»æ–‡ä»¶åä¸­æå–è®­ç»ƒæ¬¡æ•°
                # æ ¼å¼: {game_type}_dqn_training_{number}_{type}.pth
                parts = model_file.stem.split('_')
                if len(parts) >= 4 and parts[0] == self.game_type and parts[1] == "dqn" and parts[2] == "training":
                    try:
                        training_number = int(parts[3])
                        max_training_number = max(max_training_number, training_number)
                    except ValueError:
                        continue
        
        # è¿”å›æœ€å¤§è®­ç»ƒæ¬¡æ•°åŠ ä¸€
        return max_training_number + 1
        
    def _cleanup_old_models(self, current_training_number):
        """æ¸…ç†æ—§çš„æ¨¡å‹æ–‡ä»¶ï¼Œå°†è¿‡ç¨‹æ¨¡å‹ç§»åŠ¨åˆ°outputsç›®å½•ï¼Œåªåœ¨modelsç›®å½•ä¿ç•™æœ€ä¼˜å’Œæœ€åæ¨¡å‹"""
        if self.model_dir.exists():
            # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
            model_files = list(self.model_dir.glob("*.pth"))
            
            # ä¿ç•™åœ¨modelsç›®å½•çš„æ–‡ä»¶æ¨¡å¼
            keep_in_models_patterns = [
                f"{self.game_type}_dqn_best.pth",  # å…¨å±€æœ€ä¼˜æ¨¡å‹
                f"{self.game_type}_dqn_final.pth",  # å…¨å±€æœ€åæ¨¡å‹
                f"{self.game_type}_dqn_training_{current_training_number}_best.pth",  # å½“å‰è®­ç»ƒæœ€ä¼˜
                f"{self.game_type}_dqn_training_{current_training_number}_final.pth"  # å½“å‰è®­ç»ƒæœ€å
            ]
            
            # åˆ›å»ºoutputsç›®å½•ç”¨äºå­˜å‚¨è¿‡ç¨‹æ¨¡å‹
            process_models_dir = self.game_output_dir / "process_models"
            process_models_dir.mkdir(exist_ok=True)
            
            # å¤„ç†æ¯ä¸ªæ¨¡å‹æ–‡ä»¶
            for model_file in model_files:
                should_keep_in_models = False
                for pattern in keep_in_models_patterns:
                    if model_file.name == pattern:
                        should_keep_in_models = True
                        break
                
                if not should_keep_in_models:
                    try:
                        # å°†è¿‡ç¨‹æ¨¡å‹ç§»åŠ¨åˆ°outputsç›®å½•
                        target_path = process_models_dir / model_file.name
                        import shutil
                        shutil.move(str(model_file), str(target_path))
                        print(f"ç§»åŠ¨è¿‡ç¨‹æ¨¡å‹åˆ°: {target_path}")
                    except Exception as e:
                        print(f"ç§»åŠ¨æ¨¡å‹æ–‡ä»¶å¤±è´¥ {model_file.name}: {e}")
        
    def train(self, episodes, save_interval=50, model_name=None, render=False):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            episodes (int): è®­ç»ƒepisodeæ•°é‡
            save_interval (int): ä¿å­˜é—´éš”
            model_name (str): æ¨¡å‹åç§°
            render (bool): æ˜¯å¦åœ¨è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£
        """
        print(f"å¼€å§‹è®­ç»ƒ {self.game_name} æ¨¡å‹...")
        print(f"è®­ç»ƒå‚æ•°: episodes={episodes}, save_interval={save_interval}")
        
        # è·å–è®­ç»ƒæ¬¡æ•°
        training_number = self._get_training_number()
        print(f"è¿™æ˜¯ç¬¬ {training_number} æ¬¡è®­ç»ƒ")
        
        # æ¸…ç†æ—§æ¨¡å‹
        self._cleanup_old_models(training_number)
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        best_reward = float('-inf')
        best_episode = 0
        
        # è®­ç»ƒå¾ªç¯
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            reset_result = self.env.reset()
            if isinstance(reset_result, tuple):
                state = reset_result[0]  # æ–°ç‰ˆæœ¬gymnasiumè¿”å›(state, info)
            else:
                state = reset_result  # æ—§ç‰ˆæœ¬ç›´æ¥è¿”å›state
                
            total_reward = 0
            total_loss = 0
            steps = 0
            success = False
            
            while True:
                if render:
                    self.env.render()
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.choose_action(state)
                step_result = self.env.step(action)
                
                # å¤„ç†stepè¿”å›å€¼
                if len(step_result) == 5:  # æ–°ç‰ˆæœ¬gymnasium: (next_state, reward, terminated, truncated, info)
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:  # æ—§ç‰ˆæœ¬: (next_state, reward, done, info)
                    next_state, reward, done, info = step_result
                
                # å­˜å‚¨ç»éªŒ
                self.agent.store_transition(state, action, reward, next_state)
                
                # å­¦ä¹ 
                if self.agent.point > 32:
                    loss = self.agent.learn()
                    total_loss += loss
                
                state = next_state
                total_reward += reward
                steps += 1
                
                # æ¯100æ­¥è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if steps % 100 == 0:
                    current_loss = total_loss / max(steps, 1)
                    print(f"  Episode {episode + 1}, Step {steps}: Reward={total_reward:.2f}, Loss={current_loss:.4f}")
                
                if done:
                    success = self._is_success(episode, steps, total_reward)
                    break
            
            # æ·»åŠ åˆ°æ”¶æ•›åˆ†æå™¨
            avg_loss = total_loss / max(steps, 1)
            self.convergence_analyzer.add_episode_data(
                episode=episode,
                reward=total_reward,
                loss=avg_loss,
                steps=steps,
                success=success,
                epsilon=0.9  # è¿™é‡Œå¯ä»¥æ ¹æ®episodeè°ƒæ•´
            )
            
            # æ¯ä¸ªå›åˆéƒ½è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            print(f"Episode {episode + 1}/{episodes}: "
                  f"Reward={total_reward:.2f}, "
                  f"Steps={steps}, "
                  f"AvgLoss={avg_loss:.4f}, "
                  f"Success={success}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä¼˜æ¨¡å‹
            if total_reward > best_reward:
                best_reward = total_reward
                best_episode = episode + 1
                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                best_model_name = f"{self.game_type}_dqn_training_{training_number}_best.pth"
                best_model_path = self.model_dir / best_model_name
                self.agent.save_model(str(best_model_path))
                # åœ¨æ¨¡å‹æ–‡ä»¶ä¸­æ·»åŠ å¥–åŠ±ä¿¡æ¯
                checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
                checkpoint['best_reward'] = best_reward
                checkpoint['best_episode'] = best_episode
                checkpoint['training_number'] = training_number
                torch.save(checkpoint, best_model_path)
                print(f"  ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Episode {episode + 1}, Reward: {total_reward:.2f}")
            
            # å®šæœŸä¿å­˜
            if (episode + 1) % save_interval == 0:
                self._save_checkpoint(episode + 1, training_number)
                self._generate_reports(episode + 1)
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: Episode {episode + 1}")
        
        # æœ€ç»ˆä¿å­˜
        final_model_name = f"{self.game_type}_dqn_training_{training_number}_final.pth"
        final_model_path = self.model_dir / final_model_name
        self.agent.save_model(str(final_model_path))
        
        # æ›´æ–°å…¨å±€æœ€ä¼˜å’Œæœ€åæ¨¡å‹
        global_best_model_path = self.model_dir / f"{self.game_type}_dqn_best.pth"
        global_final_model_path = self.model_dir / f"{self.game_type}_dqn_final.pth"
        
        # å¤åˆ¶å½“å‰è®­ç»ƒçš„æœ€ä¼˜æ¨¡å‹åˆ°å…¨å±€æœ€ä¼˜ï¼ˆå¦‚æœæ›´å¥½ï¼‰
        if best_reward > self._get_global_best_reward():
            import shutil
            shutil.copy2(best_model_path, global_best_model_path)
            # åœ¨å…¨å±€æœ€ä¼˜æ¨¡å‹ä¸­æ·»åŠ å¥–åŠ±ä¿¡æ¯
            checkpoint = torch.load(global_best_model_path, map_location='cpu', weights_only=False)
            checkpoint['best_reward'] = best_reward
            checkpoint['best_episode'] = best_episode
            checkpoint['training_number'] = training_number
            torch.save(checkpoint, global_best_model_path)
            print(f"  ğŸŒŸ æ›´æ–°å…¨å±€æœ€ä¼˜æ¨¡å‹ï¼è®­ç»ƒ {training_number}, Episode {best_episode}, Reward: {best_reward:.2f}")
        
        # å¤åˆ¶å½“å‰è®­ç»ƒçš„æœ€åæ¨¡å‹åˆ°å…¨å±€æœ€å
        import shutil
        shutil.copy2(final_model_path, global_final_model_path)
        # åœ¨å…¨å±€æœ€åæ¨¡å‹ä¸­æ·»åŠ è®­ç»ƒä¿¡æ¯
        checkpoint = torch.load(global_final_model_path, map_location='cpu', weights_only=False)
        checkpoint['training_number'] = training_number
        checkpoint['final_episode'] = episodes
        torch.save(checkpoint, global_final_model_path)
        print(f"  ğŸ“ æ›´æ–°å…¨å±€æœ€åæ¨¡å‹ï¼è®­ç»ƒ {training_number}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_reports(episodes)
        
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä¼˜æ¨¡å‹: {best_model_name} (Episode {best_episode}, Reward: {best_reward:.2f})")
        print(f"æœ€åæ¨¡å‹: {final_model_name}")
        print(f"å…¨å±€æœ€ä¼˜: {self.game_type}_dqn_best.pth")
        print(f"å…¨å±€æœ€å: {self.game_type}_dqn_final.pth")
        
    def _get_global_best_reward(self):
        """è·å–å…¨å±€æœ€ä¼˜æ¨¡å‹çš„å¥–åŠ±å€¼"""
        global_best_model_path = self.model_dir / f"{self.game_type}_dqn_best.pth"
        if global_best_model_path.exists():
            try:
                checkpoint = torch.load(global_best_model_path, map_location='cpu', weights_only=False)
                if 'best_reward' in checkpoint:
                    return checkpoint['best_reward']
            except:
                pass
        return float('-inf')
        
    def _is_success(self, episode, steps, reward):
        """åˆ¤æ–­æ˜¯å¦æˆåŠŸ"""
        if self.game_type == "maze":
            return reward > 0  # è¿·å®«æ¸¸æˆæœ‰å¥–åŠ±å°±ç®—æˆåŠŸ
        elif self.game_type == "cartpole":
            return steps >= 195  # CartPoleæˆåŠŸæ ‡å‡†
        return False
        
    def _save_checkpoint(self, episode, training_number):
        """ä¿å­˜æ£€æŸ¥ç‚¹æ•°æ®"""
        # ç¡®ä¿convergence_analysisç›®å½•å­˜åœ¨
        convergence_dir = self.game_output_dir / "convergence_analysis"
        convergence_dir.mkdir(exist_ok=True)
        
        self.convergence_analyzer.save_analysis_data(
            str(convergence_dir / f"convergence_data_episode_{episode}.json")
        )
        
    def _generate_reports(self, episode):
        """ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨"""
        # ç”Ÿæˆæ”¶æ•›æŠ¥å‘Š
        report = self.convergence_analyzer.generate_convergence_report()
        report_path = self.game_output_dir / "reports" / f"convergence_report_episode_{episode}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # ç”Ÿæˆå›¾è¡¨ï¼ˆä¸å¼¹å‡ºçª—å£ï¼Œåªä¿å­˜æ–‡ä»¶ï¼‰
        data_file = str(self.game_output_dir / "convergence_analysis" / f"convergence_data_episode_{episode}.json")
        if os.path.exists(data_file):
            # ç¡®ä¿plotsç›®å½•å­˜åœ¨
            plots_dir = self.game_output_dir / "plots"
            plots_dir.mkdir(exist_ok=True)
            
            plot_convergence_data(
                data_file=data_file,
                save_dir=str(plots_dir),
                show_plots=False  # ä¸å¼¹å‡ºå›¾è¡¨çª—å£
            )
        
    def inference(self, model_name=None, episodes=5):
        """
        æ¨¡å‹æ¨ç†
        
        Args:
            model_name (str): æ¨¡å‹æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä¼˜æ¨¡å‹
            episodes (int): æ¨ç†episodeæ•°é‡
        """
        print(f"å¼€å§‹æ¨ç† {self.game_name} æ¨¡å‹...")
        
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        if model_name is None:
            model_name = self._get_best_model()
            if model_name is None:
                print(f"æœªæ‰¾åˆ° {self.game_name} çš„æœ€ä¼˜æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ¨¡å‹æ–‡ä»¶")
                return
            print(f"è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹: {model_name}")
        
        model_path = self.model_dir / model_name
        if not model_path.exists():
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return
            
        # åŠ è½½æ¨¡å‹
        self.agent.load_model(str(model_path))
        
        # æ¨ç†å¾ªç¯
        total_rewards = []
        total_steps = []
        success_count = 0
        
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            reset_result = self.env.reset()
            if isinstance(reset_result, tuple):
                state = reset_result[0]  # æ–°ç‰ˆæœ¬gymnasiumè¿”å›(state, info)
            else:
                state = reset_result  # æ—§ç‰ˆæœ¬ç›´æ¥è¿”å›state
                
            total_reward = 0
            steps = 0
            
            while True:
                # æ¸²æŸ“ç¯å¢ƒï¼ˆæ˜¾ç¤ºåŠ¨ç”»ï¼‰
                self.env.render()
                
                # å¤„ç†pygameäº‹ä»¶ï¼Œé˜²æ­¢çª—å£å‡æ­»
                try:
                    import pygame
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            self.env.close()
                            print("ç”¨æˆ·å…³é—­äº†æ¨ç†çª—å£")
                            return
                except ImportError:
                    pass  # å¦‚æœä¸æ˜¯pygameç¯å¢ƒï¼Œå¿½ç•¥
                
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                action = self.agent.predict_action(state, epsilon=0.0)
                step_result = self.env.step(action)
                
                # å¤„ç†stepè¿”å›å€¼
                if len(step_result) == 5:  # æ–°ç‰ˆæœ¬gymnasium: (next_state, reward, terminated, truncated, info)
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:  # æ—§ç‰ˆæœ¬: (next_state, reward, done, info)
                    next_state, reward, done, info = step_result
                
                state = next_state
                total_reward += reward
                steps += 1
                
                # æ·»åŠ é€‚å½“çš„å»¶æ—¶ï¼Œè®©åŠ¨ç”»æ›´å¹³æ»‘
                try:
                    import time
                    time.sleep(0.05)  # 50mså»¶æ—¶
                except ImportError:
                    pass
                
                if done:
                    success = self._is_success(episode, steps, total_reward)
                    if success:
                        success_count += 1
                    break
            
            total_rewards.append(total_reward)
            total_steps.append(steps)
            
            print(f"Episode {episode + 1}: Reward={total_reward:.2f}, Steps={steps}, Success={success}")
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        avg_reward = sum(total_rewards) / len(total_rewards)
        avg_steps = sum(total_steps) / len(total_steps)
        success_rate = success_count / episodes
        
        print(f"\næ¨ç†ç»Ÿè®¡:")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        print(f"å¯ç”¨çš„ {self.game_name} æ¨¡å‹:")
        if self.model_dir.exists():
            models = list(self.model_dir.glob("*.pth"))
            if models:
                # æŒ‰æ–‡ä»¶åæ’åº
                models.sort(key=lambda x: x.name)
                
                # åˆ†ç±»æ˜¾ç¤º
                print("  å…¨å±€æ¨¡å‹:")
                global_models = [m for m in models if not m.name.startswith(f"{self.game_type}_dqn_training_")]
                for model in global_models:
                    try:
                        checkpoint = torch.load(model, map_location='cpu', weights_only=False)
                        if 'best_reward' in checkpoint:
                            print(f"    - {model.name} (æœ€ä¼˜å¥–åŠ±: {checkpoint['best_reward']:.2f}, è®­ç»ƒ: {checkpoint.get('training_number', 'N/A')})")
                        elif 'training_number' in checkpoint:
                            print(f"    - {model.name} (è®­ç»ƒ: {checkpoint['training_number']}, Episode: {checkpoint.get('final_episode', 'N/A')})")
                        else:
                            print(f"    - {model.name}")
                    except:
                        print(f"    - {model.name}")
                
                print("  è®­ç»ƒå†å²æ¨¡å‹:")
                training_models = [m for m in models if m.name.startswith(f"{self.game_type}_dqn_training_")]
                for model in training_models:
                    try:
                        checkpoint = torch.load(model, map_location='cpu', weights_only=False)
                        if 'best_reward' in checkpoint:
                            print(f"    - {model.name} (æœ€ä¼˜å¥–åŠ±: {checkpoint['best_reward']:.2f}, Episode: {checkpoint.get('best_episode', 'N/A')})")
                        else:
                            print(f"    - {model.name}")
                    except:
                        print(f"    - {model.name}")
            else:
                print("  æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        else:
            print("  æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            
    def list_outputs(self):
        """åˆ—å‡ºè¾“å‡ºæ–‡ä»¶"""
        print(f"{self.game_name} è¾“å‡ºæ–‡ä»¶:")
        
        # æ”¶æ•›åˆ†æ
        convergence_dir = self.game_output_dir / "convergence_analysis"
        if convergence_dir.exists():
            files = list(convergence_dir.glob("*.json"))
            if files:
                print("  æ”¶æ•›åˆ†ææ•°æ®:")
                for file in files:
                    print(f"    - {file.name}")
            else:
                print("  æ”¶æ•›åˆ†ææ•°æ®: æ— ")
        else:
            print("  æ”¶æ•›åˆ†ææ•°æ®: ç›®å½•ä¸å­˜åœ¨")
                    
        # å›¾è¡¨
        plots_dir = self.game_output_dir / "plots"
        if plots_dir.exists():
            files = list(plots_dir.glob("*.png"))
            if files:
                print("  å›¾è¡¨æ–‡ä»¶:")
                for file in files:
                    print(f"    - {file.name}")
            else:
                print("  å›¾è¡¨æ–‡ä»¶: æ— ")
        else:
            print("  å›¾è¡¨æ–‡ä»¶: ç›®å½•ä¸å­˜åœ¨")
                    
        # æŠ¥å‘Š
        reports_dir = self.game_output_dir / "reports"
        if reports_dir.exists():
            files = list(reports_dir.glob("*.txt"))
            if files:
                print("  æŠ¥å‘Šæ–‡ä»¶:")
                for file in files:
                    print(f"    - {file.name}")
            else:
                print("  æŠ¥å‘Šæ–‡ä»¶: æ— ")
        else:
            print("  æŠ¥å‘Šæ–‡ä»¶: ç›®å½•ä¸å­˜åœ¨")
        
        # è¿‡ç¨‹æ¨¡å‹
        process_models_dir = self.game_output_dir / "process_models"
        if process_models_dir.exists():
            files = list(process_models_dir.glob("*.pth"))
            if files:
                print("  è¿‡ç¨‹æ¨¡å‹:")
                for file in files:
                    print(f"    - {file.name}")
            else:
                print("  è¿‡ç¨‹æ¨¡å‹: æ— ")
        else:
            print("  è¿‡ç¨‹æ¨¡å‹: ç›®å½•ä¸å­˜åœ¨")

    def _get_best_model(self):
        """è·å–æœ€ä¼˜æ¨¡å‹æ–‡ä»¶å"""
        if not self.model_dir.exists():
            return None
            
        # é¦–å…ˆå°è¯•æŸ¥æ‰¾å…¨å±€æœ€ä¼˜æ¨¡å‹
        best_model_path = self.model_dir / f"{self.game_type}_dqn_best.pth"
        if best_model_path.exists():
            return best_model_path.name
            
        # å¦‚æœæ²¡æœ‰å…¨å±€æœ€ä¼˜æ¨¡å‹ï¼ŒæŸ¥æ‰¾è®­ç»ƒå†å²ä¸­çš„æœ€ä¼˜æ¨¡å‹
        pattern = f"{self.game_type}_dqn_training_*_best.pth"
        training_best_models = list(self.model_dir.glob(pattern))
        
        if training_best_models:
            # æ‰¾åˆ°è®­ç»ƒæ¬¡æ•°æœ€å¤§çš„æœ€ä¼˜æ¨¡å‹
            max_training_number = 0
            best_model = None
            
            for model_path in training_best_models:
                parts = model_path.stem.split('_')
                if len(parts) >= 4:
                    try:
                        training_number = int(parts[3])
                        if training_number > max_training_number:
                            max_training_number = training_number
                            best_model = model_path
                    except ValueError:
                        continue
            
            if best_model:
                return best_model.name
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•æŸ¥æ‰¾æœ€åæ¨¡å‹
        final_model_path = self.model_dir / f"{self.game_type}_dqn_final.pth"
        if final_model_path.exists():
            return final_model_path.name
            
        return None


def normalize_input(input_str, choices, input_type):
    """
    æ ‡å‡†åŒ–è¾“å…¥ï¼Œæ”¯æŒå®Œæ•´åç§°å’Œé¦–å­—æ¯ç®€å†™
    
    Args:
        input_str (str): ç”¨æˆ·è¾“å…¥
        choices (list): å¯é€‰å€¼åˆ—è¡¨
        input_type (str): è¾“å…¥ç±»å‹æè¿°
    
    Returns:
        str: æ ‡å‡†åŒ–åçš„å€¼
    """
    if input_str in choices:
        return input_str
    
    # å°è¯•é¦–å­—æ¯åŒ¹é…
    for choice in choices:
        if choice.startswith(input_str.lower()):
            return choice
    
    # å°è¯•é¦–å­—æ¯ç»„åˆåŒ¹é…ï¼ˆå¦‚ 'cp' åŒ¹é… 'cartpole'ï¼‰
    input_lower = input_str.lower()
    for choice in choices:
        # æå–é¦–å­—æ¯
        initials = ''.join(word[0] for word in choice.split())
        if initials == input_lower:
            return choice
    
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼ŒæŠ›å‡ºé”™è¯¯
    raise ValueError(f"æ— æ•ˆçš„{input_type}: '{input_str}'ã€‚å¯é€‰å€¼: {', '.join(choices)}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DQNé¡¹ç›®ä¸»ç¨‹åº")
    parser.add_argument("--game", "-g", required=True, 
                       help="æ¸¸æˆç±»å‹ (maze/m æˆ– cartpole/cp)")
    parser.add_argument("--mode", "-m", required=True,
                       help="è¿è¡Œæ¨¡å¼ (train/t, inference/i, list-models/lm, list-outputs/lo)")
    parser.add_argument("--episodes", "-e", type=int, default=400,
                       help="è®­ç»ƒæˆ–æ¨ç†çš„episodeæ•°é‡ (é»˜è®¤: 400)")
    parser.add_argument("--model", "-M", type=str,
                       help="æ¨¡å‹æ–‡ä»¶å")
    parser.add_argument("--output-dir", "-o", type=str, default="outputs",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: outputs)")
    parser.add_argument("--model-dir", "-d", type=str, default="models",
                       help="æ¨¡å‹ç›®å½• (é»˜è®¤: models)")
    parser.add_argument("--save-interval", "-s", type=int, default=50,
                       help="ä¿å­˜é—´éš” (é»˜è®¤: 50)")
    parser.add_argument("--render", action="store_true", default=True, help="è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # æ ‡å‡†åŒ–æ¸¸æˆç±»å‹è¾“å…¥
    game_choices = ["maze", "cartpole"]
    try:
        game_type = normalize_input(args.game, game_choices, "æ¸¸æˆç±»å‹")
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print("æ”¯æŒçš„æ¸¸æˆç±»å‹:")
        print("  maze (m) - è¿·å®«æ¸¸æˆ")
        print("  cartpole (cp) - å€’ç«‹æ‘†æ¸¸æˆ")
        return
    
    # æ ‡å‡†åŒ–æ¨¡å¼è¾“å…¥
    mode_choices = ["train", "inference", "list-models", "list-outputs"]
    try:
        mode = normalize_input(args.mode, mode_choices, "è¿è¡Œæ¨¡å¼")
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print("æ”¯æŒçš„è¿è¡Œæ¨¡å¼:")
        print("  train (t) - è®­ç»ƒæ¨¡å‹")
        print("  inference (i) - æ¨ç†æ¨¡å‹")
        print("  list-models (lm) - åˆ—å‡ºæ¨¡å‹")
        print("  list-outputs (lo) - åˆ—å‡ºè¾“å‡º")
        return
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    if mode == "train":
        start_training(
            game_type=game_type,
            episodes=args.episodes,
            save_interval=args.save_interval,
            output_dir=args.output_dir,
            model_dir=args.model_dir,
            render=args.render
        )
    elif mode == "inference":
        start_inference(
            game_type=game_type,
            model_name=args.model,  # å¯ä»¥ä¸ºNoneï¼Œæ­¤æ—¶ä½¿ç”¨æœ€ä¼˜æ¨¡å‹
            episodes=args.episodes,
            output_dir=args.output_dir,
            model_dir=args.model_dir
        )
    elif mode == "list-models":
        project = DQNProject(game_type, args.output_dir, args.model_dir)
        project.list_models()
    elif mode == "list-outputs":
        project = DQNProject(game_type, args.output_dir, args.model_dir)
        project.list_outputs()


if __name__ == "__main__":
    main() 