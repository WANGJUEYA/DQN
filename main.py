#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DQNé¡¹ç›®ä¸»ç¨‹åº
æ”¯æŒå¤šç§æ¸¸æˆç¯å¢ƒçš„è®­ç»ƒå’Œæ¨ç†
"""

import argparse
import os
import sys
import json
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from games.Maze.MazeAgent import MazeAgent
from games.CartPole.CartPole import DQN
from framework.convergence_analysis import ConvergenceAnalyzer
from framework.plot_convergence import plot_convergence_data


def start_training(game_type, episodes=1000, save_interval=50, output_dir="outputs", model_dir="models", render=False):
    """
    å¯åŠ¨æ¨¡å‹è®­ç»ƒ
    
    Args:
        game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
        episodes (int): è®­ç»ƒepisodeæ•°é‡
        save_interval (int): ä¿å­˜é—´éš”
        output_dir (str): è¾“å‡ºç›®å½•
        model_dir (str): æ¨¡å‹ç›®å½•
        render (bool): æ˜¯å¦åœ¨è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£
    """
    print(f"ğŸš€ å¯åŠ¨ {game_type} æ¨¡å‹è®­ç»ƒ...")
    project = DQNProject(game_type, output_dir, model_dir)
    project.train(episodes, save_interval, render=render)
    print(f"âœ… {game_type} æ¨¡å‹è®­ç»ƒå®Œæˆï¼")


def start_inference(game_type, model_name, episodes=5, output_dir="outputs", model_dir="models"):
    """
    å¯åŠ¨æ¨¡å‹æ¨ç†
    
    Args:
        game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
        model_name (str): æ¨¡å‹æ–‡ä»¶å
        episodes (int): æ¨ç†episodeæ•°é‡
        output_dir (str): è¾“å‡ºç›®å½•
        model_dir (str): æ¨¡å‹ç›®å½•
    """
    print(f"ğŸ” å¯åŠ¨ {game_type} æ¨¡å‹æ¨ç†...")
    project = DQNProject(game_type, output_dir, model_dir)
    project.inference(model_name, episodes)
    print(f"âœ… {game_type} æ¨¡å‹æ¨ç†å®Œæˆï¼")


class DQNProject:
    """DQNé¡¹ç›®ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, game_type, output_dir="outputs", model_dir="models"):
        """
        åˆå§‹åŒ–é¡¹ç›®æ§åˆ¶å™¨
        
        Args:
            game_type (str): æ¸¸æˆç±»å‹ ('maze' æˆ– 'cartpole')
            output_dir (str): è¾“å‡ºç›®å½•
            model_dir (str): æ¨¡å‹ç›®å½•
        """
        self.game_type = game_type.lower()
        self.output_dir = Path(output_dir)
        self.model_dir = Path(model_dir)
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self._create_directories()
        
        # åˆå§‹åŒ–æ¸¸æˆç‰¹å®šçš„ç»„ä»¶
        self._init_game_components()
        
        # åŠ è½½è®­ç»ƒè®¡æ•°å™¨
        self.training_counter = self._load_training_counter()
        
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        # æ¸¸æˆç‰¹å®šçš„è¾“å‡ºç›®å½•
        self.game_output_dir = self.output_dir / self.game_type
        self.game_output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸¸æˆç‰¹å®šçš„æ¨¡å‹ç›®å½•
        self.game_model_dir = self.model_dir / self.game_type
        self.game_model_dir.mkdir(parents=True, exist_ok=True)
        
        # å­ç›®å½•
        (self.game_output_dir / "convergence_analysis").mkdir(exist_ok=True)
        (self.game_output_dir / "plots").mkdir(exist_ok=True)
        (self.game_output_dir / "logs").mkdir(exist_ok=True)
        (self.game_output_dir / "reports").mkdir(exist_ok=True)
        
    def _init_game_components(self):
        """åˆå§‹åŒ–æ¸¸æˆç‰¹å®šçš„ç»„ä»¶"""
        if self.game_type == "maze":
            from games.Maze.MazeEnv import DEFAULT_MAZE, MazeEnv
            self.env = MazeEnv(DEFAULT_MAZE)
            self.agent = MazeAgent()
            self.game_name = "Maze"
        elif self.game_type == "cartpole":
            import gymnasium as gym
            # CartPoleç¯å¢ƒéœ€è¦æŒ‡å®šæ¸²æŸ“æ¨¡å¼
            self.env = gym.make('CartPole-v1', render_mode='human').unwrapped
            self.agent = DQN()
            self.game_name = "CartPole"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¸¸æˆç±»å‹: {self.game_type}")
            
        # åˆå§‹åŒ–æ”¶æ•›åˆ†æå™¨
        self.convergence_analyzer = ConvergenceAnalyzer()
        
    def _load_training_counter(self):
        """åŠ è½½è®­ç»ƒè®¡æ•°å™¨"""
        counter_file = Path("training_counter.json")
        if counter_file.exists():
            with open(counter_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {"maze": 0, "cartpole": 0}
            
    def _save_training_counter(self):
        """ä¿å­˜è®­ç»ƒè®¡æ•°å™¨"""
        counter_file = Path("training_counter.json")
        with open(counter_file, 'w', encoding='utf-8') as f:
            json.dump(self.training_counter, f, ensure_ascii=False, indent=2)
            
    def _get_training_number(self):
        """è·å–å½“å‰è®­ç»ƒæ¬¡æ•°å¹¶é€’å¢"""
        self.training_counter[self.game_type] += 1
        self._save_training_counter()
        return self.training_counter[self.game_type]
        
    def _cleanup_old_models(self, current_training_number):
        """æ¸…ç†æ—§çš„æ¨¡å‹æ–‡ä»¶ï¼Œåªä¿ç•™æœ€ä¼˜æ¨¡å‹å’Œæœ€åæ¨¡å‹"""
        if self.game_model_dir.exists():
            # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
            model_files = list(self.game_model_dir.glob("*.pth"))
            
            # ä¿ç•™çš„æ–‡ä»¶æ¨¡å¼
            keep_patterns = [
                f"{self.game_type}_dqn_best.pth",  # æœ€ä¼˜æ¨¡å‹
                f"{self.game_type}_dqn_final.pth",  # æœ€åæ¨¡å‹
                f"{self.game_type}_dqn_training_{current_training_number}_best.pth",  # å½“å‰è®­ç»ƒæœ€ä¼˜
                f"{self.game_type}_dqn_training_{current_training_number}_final.pth"  # å½“å‰è®­ç»ƒæœ€å
            ]
            
            # åˆ é™¤ä¸éœ€è¦çš„æ–‡ä»¶
            for model_file in model_files:
                should_keep = False
                for pattern in keep_patterns:
                    if model_file.name == pattern:
                        should_keep = True
                        break
                
                if not should_keep:
                    try:
                        model_file.unlink()
                        print(f"åˆ é™¤æ—§æ¨¡å‹æ–‡ä»¶: {model_file.name}")
                    except Exception as e:
                        print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {model_file.name}: {e}")
        
    def train(self, episodes, save_interval=50, model_name=None, render=False):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            episodes (int): è®­ç»ƒepisodeæ•°é‡
            save_interval (int): ä¿å­˜é—´éš”
            model_name (str): æ¨¡å‹åç§°
            render (bool): æ˜¯å¦åœ¨è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£
        """
        print(f"å¼€å§‹è®­ç»ƒ {self.game_name} æ¨¡å‹...")
        print(f"è®­ç»ƒå‚æ•°: episodes={episodes}, save_interval={save_interval}")
        
        # è·å–è®­ç»ƒæ¬¡æ•°
        training_number = self._get_training_number()
        print(f"è¿™æ˜¯ç¬¬ {training_number} æ¬¡è®­ç»ƒ")
        
        # æ¸…ç†æ—§æ¨¡å‹
        self._cleanup_old_models(training_number)
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        best_reward = float('-inf')
        best_episode = 0
        
        # è®­ç»ƒå¾ªç¯
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            reset_result = self.env.reset()
            if isinstance(reset_result, tuple):
                state = reset_result[0]  # æ–°ç‰ˆæœ¬gymnasiumè¿”å›(state, info)
            else:
                state = reset_result  # æ—§ç‰ˆæœ¬ç›´æ¥è¿”å›state
                
            total_reward = 0
            total_loss = 0
            steps = 0
            success = False
            
            while True:
                if render:
                    self.env.render()
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.choose_action(state)
                step_result = self.env.step(action)
                
                # å¤„ç†stepè¿”å›å€¼
                if len(step_result) == 5:  # æ–°ç‰ˆæœ¬gymnasium: (next_state, reward, terminated, truncated, info)
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:  # æ—§ç‰ˆæœ¬: (next_state, reward, done, info)
                    next_state, reward, done, info = step_result
                
                # å­˜å‚¨ç»éªŒ
                self.agent.store_transition(state, action, reward, next_state)
                
                # å­¦ä¹ 
                if self.agent.point > 32:
                    loss = self.agent.learn()
                    total_loss += loss
                
                state = next_state
                total_reward += reward
                steps += 1
                
                # æ¯100æ­¥è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if steps % 100 == 0:
                    current_loss = total_loss / max(steps, 1)
                    print(f"  Episode {episode + 1}, Step {steps}: Reward={total_reward:.2f}, Loss={current_loss:.4f}")
                
                if done:
                    success = self._is_success(episode, steps, total_reward)
                    break
            
            # æ·»åŠ åˆ°æ”¶æ•›åˆ†æå™¨
            avg_loss = total_loss / max(steps, 1)
            self.convergence_analyzer.add_episode_data(
                episode=episode,
                reward=total_reward,
                loss=avg_loss,
                steps=steps,
                success=success,
                epsilon=0.9  # è¿™é‡Œå¯ä»¥æ ¹æ®episodeè°ƒæ•´
            )
            
            # æ¯ä¸ªå›åˆéƒ½è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            print(f"Episode {episode + 1}/{episodes}: "
                  f"Reward={total_reward:.2f}, "
                  f"Steps={steps}, "
                  f"AvgLoss={avg_loss:.4f}, "
                  f"Success={success}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä¼˜æ¨¡å‹
            if total_reward > best_reward:
                best_reward = total_reward
                best_episode = episode + 1
                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                best_model_name = f"{self.game_type}_dqn_training_{training_number}_best.pth"
                best_model_path = self.game_model_dir / best_model_name
                self.agent.save_model(str(best_model_path))
                # åœ¨æ¨¡å‹æ–‡ä»¶ä¸­æ·»åŠ å¥–åŠ±ä¿¡æ¯
                checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
                checkpoint['best_reward'] = best_reward
                checkpoint['best_episode'] = best_episode
                checkpoint['training_number'] = training_number
                torch.save(checkpoint, best_model_path)
                print(f"  ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Episode {episode + 1}, Reward: {total_reward:.2f}")
            
            # å®šæœŸä¿å­˜
            if (episode + 1) % save_interval == 0:
                self._save_checkpoint(episode + 1, training_number)
                self._generate_reports(episode + 1)
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: Episode {episode + 1}")
        
        # æœ€ç»ˆä¿å­˜
        final_model_name = f"{self.game_type}_dqn_training_{training_number}_final.pth"
        final_model_path = self.game_model_dir / final_model_name
        self.agent.save_model(str(final_model_path))
        
        # æ›´æ–°å…¨å±€æœ€ä¼˜å’Œæœ€åæ¨¡å‹
        global_best_model_path = self.game_model_dir / f"{self.game_type}_dqn_best.pth"
        global_final_model_path = self.game_model_dir / f"{self.game_type}_dqn_final.pth"
        
        # å¤åˆ¶å½“å‰è®­ç»ƒçš„æœ€ä¼˜æ¨¡å‹åˆ°å…¨å±€æœ€ä¼˜ï¼ˆå¦‚æœæ›´å¥½ï¼‰
        if best_reward > self._get_global_best_reward():
            import shutil
            shutil.copy2(best_model_path, global_best_model_path)
            # åœ¨å…¨å±€æœ€ä¼˜æ¨¡å‹ä¸­æ·»åŠ å¥–åŠ±ä¿¡æ¯
            checkpoint = torch.load(global_best_model_path, map_location='cpu', weights_only=False)
            checkpoint['best_reward'] = best_reward
            checkpoint['best_episode'] = best_episode
            checkpoint['training_number'] = training_number
            torch.save(checkpoint, global_best_model_path)
            print(f"  ğŸŒŸ æ›´æ–°å…¨å±€æœ€ä¼˜æ¨¡å‹ï¼è®­ç»ƒ {training_number}, Episode {best_episode}, Reward: {best_reward:.2f}")
        
        # å¤åˆ¶å½“å‰è®­ç»ƒçš„æœ€åæ¨¡å‹åˆ°å…¨å±€æœ€å
        import shutil
        shutil.copy2(final_model_path, global_final_model_path)
        # åœ¨å…¨å±€æœ€åæ¨¡å‹ä¸­æ·»åŠ è®­ç»ƒä¿¡æ¯
        checkpoint = torch.load(global_final_model_path, map_location='cpu', weights_only=False)
        checkpoint['training_number'] = training_number
        checkpoint['final_episode'] = episodes
        torch.save(checkpoint, global_final_model_path)
        print(f"  ğŸ“ æ›´æ–°å…¨å±€æœ€åæ¨¡å‹ï¼è®­ç»ƒ {training_number}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_reports(episodes)
        
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä¼˜æ¨¡å‹: {best_model_name} (Episode {best_episode}, Reward: {best_reward:.2f})")
        print(f"æœ€åæ¨¡å‹: {final_model_name}")
        print(f"å…¨å±€æœ€ä¼˜: {self.game_type}_dqn_best.pth")
        print(f"å…¨å±€æœ€å: {self.game_type}_dqn_final.pth")
        
    def _get_global_best_reward(self):
        """è·å–å…¨å±€æœ€ä¼˜æ¨¡å‹çš„å¥–åŠ±å€¼"""
        global_best_model_path = self.game_model_dir / f"{self.game_type}_dqn_best.pth"
        if global_best_model_path.exists():
            try:
                checkpoint = torch.load(global_best_model_path, map_location='cpu', weights_only=False)
                if 'best_reward' in checkpoint:
                    return checkpoint['best_reward']
            except:
                pass
        return float('-inf')
        
    def _is_success(self, episode, steps, reward):
        """åˆ¤æ–­æ˜¯å¦æˆåŠŸ"""
        if self.game_type == "maze":
            return reward > 0  # è¿·å®«æ¸¸æˆæœ‰å¥–åŠ±å°±ç®—æˆåŠŸ
        elif self.game_type == "cartpole":
            return steps >= 195  # CartPoleæˆåŠŸæ ‡å‡†
        return False
        
    def _save_checkpoint(self, episode, training_number):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # ä¿å­˜æ”¶æ•›æ•°æ®
        self.convergence_analyzer.save_analysis_data(
            str(self.game_output_dir / "convergence_analysis" / f"convergence_data_episode_{episode}.json")
        )
        
    def _generate_reports(self, episode):
        """ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨"""
        # ç”Ÿæˆæ”¶æ•›æŠ¥å‘Š
        report = self.convergence_analyzer.generate_convergence_report()
        report_path = self.game_output_dir / "reports" / f"convergence_report_episode_{episode}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # ç”Ÿæˆå›¾è¡¨ï¼ˆä¸å¼¹å‡ºçª—å£ï¼Œåªä¿å­˜æ–‡ä»¶ï¼‰
        data_file = str(self.game_output_dir / "convergence_analysis" / f"convergence_data_episode_{episode}.json")
        if os.path.exists(data_file):
            plot_convergence_data(
                data_file=data_file,
                save_dir=str(self.game_output_dir / "plots"),
                show_plots=False  # ä¸å¼¹å‡ºå›¾è¡¨çª—å£
            )
        
    def inference(self, model_name, episodes=5):
        """
        æ¨¡å‹æ¨ç†
        
        Args:
            model_name (str): æ¨¡å‹æ–‡ä»¶å
            episodes (int): æ¨ç†episodeæ•°é‡
        """
        print(f"å¼€å§‹æ¨ç† {self.game_name} æ¨¡å‹...")
        
        model_path = self.game_model_dir / model_name
        if not model_path.exists():
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return
            
        # åŠ è½½æ¨¡å‹
        self.agent.load_model(str(model_path))
        
        # æ¨ç†å¾ªç¯
        total_rewards = []
        total_steps = []
        success_count = 0
        
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            reset_result = self.env.reset()
            if isinstance(reset_result, tuple):
                state = reset_result[0]  # æ–°ç‰ˆæœ¬gymnasiumè¿”å›(state, info)
            else:
                state = reset_result  # æ—§ç‰ˆæœ¬ç›´æ¥è¿”å›state
                
            total_reward = 0
            steps = 0
            
            while True:
                # æ¸²æŸ“ç¯å¢ƒï¼ˆæ˜¾ç¤ºåŠ¨ç”»ï¼‰
                self.env.render()
                
                # å¤„ç†pygameäº‹ä»¶ï¼Œé˜²æ­¢çª—å£å‡æ­»
                try:
                    import pygame
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            self.env.close()
                            print("ç”¨æˆ·å…³é—­äº†æ¨ç†çª—å£")
                            return
                except ImportError:
                    pass  # å¦‚æœä¸æ˜¯pygameç¯å¢ƒï¼Œå¿½ç•¥
                
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                action = self.agent.predict_action(state, epsilon=0.0)
                step_result = self.env.step(action)
                
                # å¤„ç†stepè¿”å›å€¼
                if len(step_result) == 5:  # æ–°ç‰ˆæœ¬gymnasium: (next_state, reward, terminated, truncated, info)
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:  # æ—§ç‰ˆæœ¬: (next_state, reward, done, info)
                    next_state, reward, done, info = step_result
                
                state = next_state
                total_reward += reward
                steps += 1
                
                # æ·»åŠ é€‚å½“çš„å»¶æ—¶ï¼Œè®©åŠ¨ç”»æ›´å¹³æ»‘
                try:
                    import time
                    time.sleep(0.05)  # 50mså»¶æ—¶
                except ImportError:
                    pass
                
                if done:
                    success = self._is_success(episode, steps, total_reward)
                    if success:
                        success_count += 1
                    break
            
            total_rewards.append(total_reward)
            total_steps.append(steps)
            
            print(f"Episode {episode + 1}: Reward={total_reward:.2f}, Steps={steps}, Success={success}")
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        avg_reward = sum(total_rewards) / len(total_rewards)
        avg_steps = sum(total_steps) / len(total_steps)
        success_rate = success_count / episodes
        
        print(f"\næ¨ç†ç»Ÿè®¡:")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        print(f"å¯ç”¨çš„ {self.game_name} æ¨¡å‹:")
        if self.game_model_dir.exists():
            models = list(self.game_model_dir.glob("*.pth"))
            if models:
                # æŒ‰æ–‡ä»¶åæ’åº
                models.sort(key=lambda x: x.name)
                
                # åˆ†ç±»æ˜¾ç¤º
                print("  å…¨å±€æ¨¡å‹:")
                global_models = [m for m in models if not m.name.startswith(f"{self.game_type}_dqn_training_")]
                for model in global_models:
                    try:
                        checkpoint = torch.load(model, map_location='cpu', weights_only=False)
                        if 'best_reward' in checkpoint:
                            print(f"    - {model.name} (æœ€ä¼˜å¥–åŠ±: {checkpoint['best_reward']:.2f}, è®­ç»ƒ: {checkpoint.get('training_number', 'N/A')})")
                        elif 'training_number' in checkpoint:
                            print(f"    - {model.name} (è®­ç»ƒ: {checkpoint['training_number']}, Episode: {checkpoint.get('final_episode', 'N/A')})")
                        else:
                            print(f"    - {model.name}")
                    except:
                        print(f"    - {model.name}")
                
                print("  è®­ç»ƒå†å²æ¨¡å‹:")
                training_models = [m for m in models if m.name.startswith(f"{self.game_type}_dqn_training_")]
                for model in training_models:
                    try:
                        checkpoint = torch.load(model, map_location='cpu', weights_only=False)
                        if 'best_reward' in checkpoint:
                            print(f"    - {model.name} (æœ€ä¼˜å¥–åŠ±: {checkpoint['best_reward']:.2f}, Episode: {checkpoint.get('best_episode', 'N/A')})")
                        else:
                            print(f"    - {model.name}")
                    except:
                        print(f"    - {model.name}")
            else:
                print("  æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        else:
            print("  æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            
    def list_outputs(self):
        """åˆ—å‡ºè¾“å‡ºæ–‡ä»¶"""
        print(f"{self.game_name} è¾“å‡ºæ–‡ä»¶:")
        
        # æ”¶æ•›åˆ†æ
        convergence_dir = self.game_output_dir / "convergence_analysis"
        if convergence_dir.exists():
            files = list(convergence_dir.glob("*.json"))
            if files:
                print("  æ”¶æ•›åˆ†ææ•°æ®:")
                for file in files:
                    print(f"    - {file.name}")
                    
        # å›¾è¡¨
        plots_dir = self.game_output_dir / "plots"
        if plots_dir.exists():
            files = list(plots_dir.glob("*.png"))
            if files:
                print("  å›¾è¡¨æ–‡ä»¶:")
                for file in files:
                    print(f"    - {file.name}")
                    
        # æŠ¥å‘Š
        reports_dir = self.game_output_dir / "reports"
        if reports_dir.exists():
            files = list(reports_dir.glob("*.txt"))
            if files:
                print("  æŠ¥å‘Šæ–‡ä»¶:")
                for file in files:
                    print(f"    - {file.name}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DQNé¡¹ç›®ä¸»ç¨‹åº")
    parser.add_argument("--game", "-g", required=True, 
                       choices=["maze", "cartpole"], 
                       help="æ¸¸æˆç±»å‹ (maze/cartpole)")
    parser.add_argument("--mode", "-m", required=True,
                       choices=["train", "inference", "list-models", "list-outputs"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--episodes", "-e", type=int, default=400,
                       help="è®­ç»ƒæˆ–æ¨ç†çš„episodeæ•°é‡ (é»˜è®¤: 400)")
    parser.add_argument("--model", "-M", type=str,
                       help="æ¨¡å‹æ–‡ä»¶å")
    parser.add_argument("--output-dir", "-o", type=str, default="outputs",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: outputs)")
    parser.add_argument("--model-dir", "-d", type=str, default="models",
                       help="æ¨¡å‹ç›®å½• (é»˜è®¤: models)")
    parser.add_argument("--save-interval", "-s", type=int, default=50,
                       help="ä¿å­˜é—´éš” (é»˜è®¤: 50)")
    parser.add_argument("--render", action="store_true", help="è®­ç»ƒæ—¶æ˜¾ç¤ºå¯è§†åŒ–åŠ¨ç”»çª—å£")
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.mode == "train":
        start_training(
            game_type=args.game,
            episodes=args.episodes,
            save_interval=args.save_interval,
            output_dir=args.output_dir,
            model_dir=args.model_dir,
            render=args.render
        )
    elif args.mode == "inference":
        if not args.model:
            print("æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®šæ¨¡å‹æ–‡ä»¶ (--model)")
            return
        start_inference(
            game_type=args.game,
            model_name=args.model,
            episodes=args.episodes,
            output_dir=args.output_dir,
            model_dir=args.model_dir
        )
    elif args.mode == "list-models":
        project = DQNProject(args.game, args.output_dir, args.model_dir)
        project.list_models()
    elif args.mode == "list-outputs":
        project = DQNProject(args.game, args.output_dir, args.model_dir)
        project.list_outputs()


if __name__ == "__main__":
    main() 