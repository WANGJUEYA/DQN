# Gym环境中DQN模型的导出和推理预测

本文档详细说明如何在Gym环境中训练DQN模型，导出模型文件，并进行推理预测。

## 目录
1. [模型训练和导出](#模型训练和导出)
2. [模型推理方法](#模型推理方法)
3. [使用示例](#使用示例)
4. [高级功能](#高级功能)

## 模型训练和导出

### 1. CartPole环境

#### 训练模型
```bash
cd CartPole
python CartPole.py
```

训练过程中会自动保存模型：
- 每50个episode保存一次：`models/cartpole_dqn_episode_X.pth`
- 训练完成后保存最终模型：`models/cartpole_dqn_final.pth`

#### 推理预测
```bash
# 使用默认模型进行推理
python CartPole.py inference

# 使用指定模型进行推理
python CartPole.py inference cartpole_dqn_final.pth

# 使用指定模型进行10次推理
python CartPole.py inference cartpole_dqn_final.pth 10
```

### 2. Maze环境

#### 训练模型
```bash
cd Maze
python MazeAgent.py
```

训练过程中会自动保存模型：
- 每5个episode保存一次：`models/maze_dqn_episode_X.pth`
- 训练完成后保存最终模型：`models/maze_dqn_final.pth`

#### 推理预测
```bash
# 使用默认模型进行推理
python MazeAgent.py inference

# 使用指定模型进行推理
python MazeAgent.py inference maze_dqn_final.pth

# 使用指定模型进行10次推理
python MazeAgent.py inference maze_dqn_final.pth 10
```

## 模型推理方法

### 方法1：使用内置推理功能

每个训练脚本都集成了推理功能，通过命令行参数控制：

```python
# 在训练脚本中添加的功能
def predict_action(self, s, epsilon=0.0):
    """推理预测函数，epsilon=0表示完全贪婪策略"""
    s = torch.unsqueeze(torch.FloatTensor(s), 0)
    if np.random.uniform() < epsilon:
        return np.random.randint(0, N_ACTIONS)
    else:
        return torch.max(self.evaluate_net.forward(s), 1)[1].data.numpy()[0]

def save_model(self, model_name="model.pth"):
    """保存模型"""
    model_path = os.path.join(MODEL_SAVE_DIR, model_name)
    torch.save({
        'evaluate_net_state_dict': self.evaluate_net.state_dict(),
        'target_net_state_dict': self.target_net.state_dict(),
        'optimizer_state_dict': self.optimizer.state_dict(),
        'hyperparameters': {
            'N_STATES': N_STATES,
            'N_ACTIONS': N_ACTIONS,
            'LR': LR,
            'GAMMA': GAMMA,
            'EPSILON': EPSILON,
            'MEMORY_CAPACITY': MEMORY_CAPACITY,
            'BATCH_SIZE': BATCH_SIZE,
            'TARGET_REPLACE_ITER': TARGET_REPLACE_ITER
        }
    }, model_path)

def load_model(self, model_name="model.pth"):
    """加载模型"""
    model_path = os.path.join(MODEL_SAVE_DIR, model_name)
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path)
        self.evaluate_net.load_state_dict(checkpoint['evaluate_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        return True
    return False
```

### 方法2：使用通用推理工具

使用独立的推理工具 `model_inference.py`：

```bash
# 基本用法
python model_inference.py <模型路径> [环境类型] [episode数量]

# 示例
python model_inference.py models/cartpole_dqn_final.pth cartpole 5
python model_inference.py models/maze_dqn_final.pth maze 10
```

## 使用示例

### 示例1：CartPole环境完整流程

```python
# 1. 训练模型
python CartPole/CartPole.py

# 2. 推理预测
python CartPole/CartPole.py inference cartpole_dqn_final.pth 5

# 3. 使用通用推理工具
python model_inference.py models/cartpole_dqn_final.pth cartpole 5
```

### 示例2：Maze环境完整流程

```python
# 1. 训练模型
python Maze/MazeAgent.py

# 2. 推理预测
python Maze/MazeAgent.py inference maze_dqn_final.pth 10

# 3. 使用通用推理工具
python model_inference.py models/maze_dqn_final.pth maze 10
```

### 示例3：自定义推理代码

```python
import torch
import numpy as np
from CartPole.CartPole import DQN, Net
import gym

# 创建环境和模型
env = gym.make('CartPole-v0')
env = env.unwrapped

# 创建DQN实例
dqn = DQN()

# 加载训练好的模型
if dqn.load_model("cartpole_dqn_final.pth"):
    # 设置推理模式
    dqn.evaluate_net.eval()
    
    # 运行推理
    state = env.reset()
    total_reward = 0
    
    for step in range(1000):
        env.render()
        
        # 预测动作（epsilon=0，完全贪婪）
        action = dqn.predict_action(state, epsilon=0.0)
        
        # 执行动作
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        
        if done:
            print(f"Episode结束，总奖励: {total_reward}")
            break
            
        state = next_state
    
    env.close()
```

## 高级功能

### 1. 模型检查点保存

训练过程中自动保存检查点，包含：
- 网络参数
- 优化器状态
- 超参数配置

### 2. 推理模式优化

推理时使用以下优化：
- `epsilon=0.0`：完全贪婪策略
- `torch.no_grad()`：禁用梯度计算
- `model.eval()`：设置为评估模式

### 3. 性能统计

推理工具提供详细的性能统计：
- 平均奖励
- 成功率
- 步数统计
- 最高/最低奖励

### 4. 多设备支持

推理工具自动检测并使用可用的设备：
- CPU：默认设备
- GPU：如果可用则自动使用

## 注意事项

1. **模型兼容性**：确保推理时使用的网络结构与训练时一致
2. **环境版本**：确保推理环境与训练环境版本一致
3. **超参数**：推理时会自动从模型文件中加载超参数
4. **内存管理**：推理完成后记得关闭环境释放资源

## 故障排除

### 常见问题

1. **模型文件不存在**
   ```
   解决方案：确保模型文件路径正确，先完成训练
   ```

2. **环境不匹配**
   ```
   解决方案：检查环境类型参数是否正确
   ```

3. **网络结构不匹配**
   ```
   解决方案：确保推理时使用的网络结构与训练时一致
   ```

### 调试技巧

1. 使用 `print` 语句检查模型加载状态
2. 验证输入状态的形状和类型
3. 检查动作空间和状态空间是否匹配
4. 使用TensorBoard查看训练过程

## 总结

通过以上方法，您可以：
1. 训练DQN模型并自动保存
2. 使用多种方式进行模型推理
3. 获得详细的性能统计
4. 在不同环境中复用训练好的模型

这些功能使得DQN模型的实际应用变得更加简单和可靠。 